"""
TSLA íŒŒì‹± ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ë° í•´ê²°

ëª©í‘œ: TSLAë¥¼ í¬í•¨í•œ ëª¨ë“  ì¢…ëª© 100% ìˆ˜ì§‘!
"""
import requests
from bs4 import BeautifulSoup
import time
import json


def debug_tsla():
    """TSLA 10-K ìˆ˜ì§‘ ë””ë²„ê¹…"""
    
    BASE_URL = "https://www.sec.gov"
    headers = {
        'User-Agent': 'Newturn Investment Platform contact@newturn.ai',
        'Accept-Encoding': 'gzip, deflate',
        'Host': 'www.sec.gov'
    }
    
    ticker = "TSLA"
    
    print("="*80)
    print(f"ğŸ” {ticker} ë””ë²„ê¹…")
    print("="*80)
    
    # 1. CIK í™•ì¸
    print("\n1ï¸âƒ£ CIK í™•ì¸...")
    cik_url = "https://www.sec.gov/files/company_tickers.json"
    response = requests.get(cik_url, headers=headers)
    time.sleep(0.11)
    
    data = response.json()
    cik = None
    
    for key, company in data.items():
        if company['ticker'].upper() == ticker:
            cik = str(company['cik_str']).zfill(10)
            print(f"âœ… CIK: {cik}")
            break
    
    if not cik:
        print("âŒ CIK not found")
        return
    
    # 2. 10-K ê²€ìƒ‰
    print("\n2ï¸âƒ£ 10-K ê²€ìƒ‰...")
    url = f"{BASE_URL}/cgi-bin/browse-edgar"
    params = {
        'action': 'getcompany',
        'CIK': cik,
        'type': '10-K',
        'dateb': '',
        'owner': 'exclude',
        'count': '3',  # ìµœê·¼ 3ê°œ í™•ì¸
    }
    
    response = requests.get(url, params=params, headers=headers)
    time.sleep(0.11)
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Filing table í™•ì¸
    table = soup.find('table', class_='tableFile2')
    
    if not table:
        print("âŒ No filing table")
        return
    
    rows = table.find_all('tr')[1:]  # Skip header
    
    print(f"âœ… Found {len(rows)} filings:")
    
    for i, row in enumerate(rows[:3], 1):
        cells = row.find_all('td')
        filing_type = cells[0].text.strip()
        filing_date = cells[3].text.strip()
        
        print(f"\n   Filing #{i}:")
        print(f"     Type: {filing_type}")
        print(f"     Date: {filing_date}")
        
        # Documents ë²„íŠ¼
        doc_button = row.find('a', {'id': 'documentsbutton'})
        
        if doc_button:
            doc_url = BASE_URL + doc_button['href']
            print(f"     Documents: {doc_url}")
            
            # Documents í˜ì´ì§€ í™•ì¸
            response2 = requests.get(doc_url, headers=headers)
            time.sleep(0.11)
            
            soup2 = BeautifulSoup(response2.content, 'html.parser')
            table2 = soup2.find('table', class_='tableFile')
            
            if table2:
                doc_rows = table2.find_all('tr')[1:]
                print(f"     Documents found: {len(doc_rows)}")
                
                # 10-K ë¬¸ì„œ ì°¾ê¸°
                for doc_row in doc_rows[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                    doc_cells = doc_row.find_all('td')
                    if len(doc_cells) >= 4:
                        seq = doc_cells[0].text.strip()
                        description = doc_cells[1].text.strip()
                        doc_type = doc_cells[3].text.strip()
                        
                        link = doc_cells[2].find('a')
                        if link:
                            href = link.get('href', '')
                            filename = link.text.strip()
                            
                            print(f"        - [{seq}] {description[:40]} ({doc_type})")
                            print(f"          File: {filename}")
                            print(f"          Href: {href[:80]}...")
                            
                            # 10-Kì¸ì§€ í™•ì¸
                            if doc_type == '10-K' or '10-K' in description:
                                print(f"          âœ… This is 10-K!")
                                
                                # URL ìƒì„±
                                if 'ix?doc=' in href:
                                    actual_path = href.split('ix?doc=')[1]
                                    final_url = BASE_URL + actual_path
                                else:
                                    final_url = BASE_URL + href
                                
                                print(f"          Final URL: {final_url}")
                                
                                # ë‹¤ìš´ë¡œë“œ ì‹œë„
                                print(f"          ğŸ“¥ Trying to download...")
                                try:
                                    response3 = requests.get(final_url, headers=headers, timeout=30)
                                    time.sleep(0.11)
                                    
                                    if response3.status_code == 200:
                                        size = len(response3.text)
                                        print(f"          âœ… Downloaded: {size:,} bytes")
                                        
                                        # ê°„ë‹¨í•œ íŒŒì‹± í…ŒìŠ¤íŠ¸
                                        if 'Item 1' in response3.text or 'ITEM 1' in response3.text:
                                            print(f"          âœ… Contains 'Item 1' - Likely valid!")
                                        else:
                                            print(f"          âš ï¸ No 'Item 1' found")
                                        
                                        return {
                                            'success': True,
                                            'url': final_url,
                                            'size': size
                                        }
                                    else:
                                        print(f"          âŒ Status: {response3.status_code}")
                                except Exception as e:
                                    print(f"          âŒ Error: {e}")
            else:
                print(f"     âš ï¸ No document table")
        else:
            print(f"     âš ï¸ No documents button")
    
    return None


if __name__ == "__main__":
    result = debug_tsla()
    
    if result:
        print(f"\n{'='*80}")
        print("âœ… TSLA 10-K ìˆ˜ì§‘ ê°€ëŠ¥!")
        print(f"URL: {result['url']}")
        print(f"Size: {result['size']:,} bytes")
        print("="*80)
    else:
        print(f"\n{'='*80}")
        print("âŒ TSLA 10-K ìˆ˜ì§‘ ì‹¤íŒ¨ - ì¶”ê°€ ì¡°ì‚¬ í•„ìš”")
        print("="*80)

