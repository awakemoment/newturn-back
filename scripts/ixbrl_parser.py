"""
iXBRL 10-K íŒŒì„œ
SECì˜ ìµœì‹  í‘œì¤€ í˜•ì‹ì¸ iXBRL(Inline XBRL)ì„ ì™„ì „íˆ íŒŒì‹±

ëª©í‘œ:
1. ì „ì²´ 10-K HTML ì¶”ì¶œ
2. ì¬ë¬´ ë°ì´í„° íƒœê·¸ íŒŒì‹±
3. Itemë³„ ì„¹ì…˜ êµ¬ì¡°í™”
4. í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
5. ì œí’ˆ/ì§€ì—­/ê²½ìŸì‚¬ ì •ë³´ ì¶”ì¶œ

ì´ê²ƒì´ ë‰´í„´ì˜ í•µì‹¬ ìì‚°!
"""
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import time


class iXBRLParser:
    """iXBRL 10-K ì™„ì „ íŒŒì„œ"""
    
    BASE_URL = "https://www.sec.gov"
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Newturn AI Investment Platform contact@newturn.ai',
            'Accept-Encoding': 'gzip, deflate',
            'Host': 'www.sec.gov'
        }
    
    def get_cik(self, ticker):
        """í‹°ì»¤ â†’ CIK"""
        url = "https://www.sec.gov/files/company_tickers.json"
        response = requests.get(url, headers=self.headers)
        time.sleep(0.11)  # SEC rate limit: 10 requests/second
        
        data = response.json()
        for key, company in data.items():
            if company['ticker'].upper() == ticker.upper():
                return str(company['cik_str']).zfill(10)
        return None
    
    def get_latest_10k(self, ticker):
        """ìµœì‹  10-K ë©”íƒ€ë°ì´í„°"""
        cik = self.get_cik(ticker)
        if not cik:
            return None
        
        print(f"âœ… {ticker} CIK: {cik}")
        
        # EDGAR Search (ê¸°ì¡´ ë°©ì‹)
        # 10-Kì™€ 10-K/A ëª¨ë‘ ê²€ìƒ‰
        url = f"{self.BASE_URL}/cgi-bin/browse-edgar"
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': '10-K',  # 10-Kì™€ 10-K/A ëª¨ë‘ í¬í•¨
            'dateb': '',  # ìµœì‹ 
            'owner': 'exclude',
            'count': '3',  # ìµœê·¼ 3ê°œ í™•ì¸ (10-K/A ê³ ë ¤)
        }
        
        response = requests.get(url, params=params, headers=self.headers)
        time.sleep(0.11)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Filing info from table
        table = soup.find('table', class_='tableFile2')
        if not table:
            return None
        
        rows = table.find_all('tr')[1:]  # Skip header
        if not rows:
            return None
        
        # 10-K ë˜ëŠ” 10-K/A ì°¾ê¸° (10-K/A ìš°ì„ )
        target_row = None
        for row in rows[:3]:
            cells = row.find_all('td')
            filing_type = cells[0].text.strip()
            if filing_type in ['10-K', '10-K/A']:
                target_row = row
                print(f"âœ… Found: {filing_type}")
                break
        
        if not target_row:
            return None
        
        cells = target_row.find_all('td')
        filing_date = cells[3].text.strip()
        
        # Documents button
        doc_button = target_row.find('a', {'id': 'documentsbutton'})
        if not doc_button:
            return None
        
        documents_url = self.BASE_URL + doc_button['href']
        
        print(f"âœ… 10-K Found: {filing_date}")
        print(f"   Documents: {documents_url}")
        
        # Get actual HTML file
        response2 = requests.get(documents_url, headers=self.headers)
        time.sleep(0.11)
        
        soup2 = BeautifulSoup(response2.content, 'html.parser')
        table2 = soup2.find('table', class_='tableFile')
        
        if not table2:
            return None
        
        # Find 10-K document (10-K ë˜ëŠ” 10-K/A)
        # ix? ë§í¬ê°€ ì•„ë‹Œ ì‹¤ì œ .htm íŒŒì¼ ì°¾ê¸°
        for row in table2.find_all('tr')[1:]:
            cells = row.find_all('td')
            if len(cells) >= 4:
                doc_type = cells[3].text.strip()
                description = cells[1].text.strip()
                
                # 10-K ë˜ëŠ” 10-K/A ëª¨ë‘ í—ˆìš©
                if doc_type in ['10-K', '10-K/A'] or '10-K' in description:
                    link = cells[2].find('a')
                    if link:
                        href = link.get('href', '')
                        primary_doc = link.text.strip()
                        
                        # ix? ë§í¬ ê±´ë„ˆë›°ê¸°, ì‹¤ì œ .htm íŒŒì¼ë§Œ
                        if 'ix?' not in href and primary_doc.endswith('.htm'):
                            doc_url = self.BASE_URL + href
                            
                            print(f"   Document: {primary_doc}")
                            print(f"   Type: {doc_type}")
                            print(f"   URL: {doc_url}")
                            
                            return {
                                'ticker': ticker,
                                'cik': cik,
                                'filing_date': filing_date,
                                'document_url': doc_url,
                                'primary_document': primary_doc,
                                'filing_type': doc_type,
                            }
        
        # ì‹¤ì œ .htm ëª» ì°¾ìœ¼ë©´ ì•„ë¬´ê±°ë‚˜ (10-K/A í¬í•¨)
        print("   âš ï¸ No direct .htm found, trying any 10-K or 10-K/A...")
        for row in table2.find_all('tr')[1:]:
            cells = row.find_all('td')
            if len(cells) >= 4:
                doc_type = cells[3].text.strip()
                description = cells[1].text.strip()
                
                # 10-K ë˜ëŠ” 10-K/A
                if doc_type in ['10-K', '10-K/A'] or '10-K' in description:
                    link = cells[2].find('a')
                    if link:
                        href = link.get('href', '')
                        # ix? ë§í¬ë©´ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜
                        if 'ix?doc=' in href:
                            # ix?doc=/Archives/... â†’ ì§ì ‘ ê²½ë¡œ
                            actual_path = href.split('ix?doc=')[1]
                            doc_url = self.BASE_URL + actual_path
                        else:
                            doc_url = self.BASE_URL + href
                        
                        primary_doc = link.text.strip()
                        
                        print(f"   Document (converted): {primary_doc}")
                        print(f"   Type: {doc_type}")
                        print(f"   URL: {doc_url}")
                        
                        return {
                            'ticker': ticker,
                            'cik': cik,
                            'filing_date': filing_date,
                            'document_url': doc_url,
                            'primary_document': primary_doc,
                            'filing_type': doc_type,
                        }
        
        return None
    
    def download_10k_html(self, doc_url):
        """10-K HTML ë‹¤ìš´ë¡œë“œ"""
        print(f"\nğŸ“¥ Downloading: {doc_url}")
        
        response = requests.get(doc_url, headers=self.headers)
        time.sleep(0.11)
        
        html = response.text
        
        print(f"âœ… Downloaded: {len(html):,} bytes ({len(html)/1024:.1f} KB)")
        
        return html
    
    def parse_ixbrl_10k(self, html):
        """iXBRL 10-K ì™„ì „ íŒŒì‹±"""
        
        print(f"\nğŸ” Parsing iXBRL 10-K...")
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. iXBRL íƒœê·¸ ì œê±°í•˜ê³  ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ
        # ix:header, ix:hidden, ix:nonfraction ë“± ì œê±°
        for tag in soup.find_all(['ix:header', 'ix:hidden']):
            tag.decompose()
        
        # 2. ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # <body> ì•ˆì˜ ëª¨ë“  í…ìŠ¤íŠ¸
        body = soup.find('body')
        
        if not body:
            print("âš ï¸ No <body> tag found, using whole document")
            text = soup.get_text(separator='\n', strip=False)
        else:
            text = body.get_text(separator='\n', strip=False)
        
        # 3. ì •ì œ
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        # íƒ­ ì œê±°
        text = re.sub(r'\t+', ' ', text)
        # ì¤„ ë ê³µë°± ì œê±°
        lines = [line.rstrip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        print(f"âœ… Extracted text: {len(text):,} characters")
        print(f"   Lines: {len(lines):,}")
        
        # 4. êµ¬ì¡° ë¶„ì„
        structure = self.analyze_document_structure(text)
        
        # 5. Itemë³„ ì„¹ì…˜ ì¶”ì¶œ
        sections = self.extract_all_sections(text)
        
        return {
            'text_length': len(text),
            'line_count': len(lines),
            'structure': structure,
            'sections': sections,
        }
    
    def analyze_document_structure(self, text):
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ (ëª©ì°¨ ì°¾ê¸°)"""
        
        print(f"\nğŸ“‘ Analyzing document structure...")
        
        # TABLE OF CONTENTS ì°¾ê¸°
        toc_patterns = [
            r'TABLE\s+OF\s+CONTENTS',
            r'Index\s+to\s+Financial\s+Statements',
        ]
        
        toc_pos = None
        for pattern in toc_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                toc_pos = match.start()
                print(f"âœ… Found TOC at position {toc_pos:,}")
                break
        
        # Item ëª©ë¡ ì°¾ê¸°
        item_pattern = r'(?:PART|Part)\s+([IVX]+).*?(?:ITEM|Item)\s+(\d+[A-Z]?)'
        
        items = []
        for match in re.finditer(item_pattern, text[:50000] if toc_pos else text):  # ì²˜ìŒ 50KBë§Œ
            part = match.group(1)
            item = match.group(2)
            items.append(f"Part {part} - Item {item}")
        
        if items:
            print(f"âœ… Found {len(items)} items in document")
            for item in items[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                print(f"   - {item}")
        
        return {
            'has_toc': toc_pos is not None,
            'toc_position': toc_pos,
            'items_found': items,
        }
    
    def extract_all_sections(self, text):
        """ëª¨ë“  ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ"""
        
        print(f"\nğŸ“„ Extracting sections...")
        
        sections = {}
        
        # Item 1: Business
        item1 = self.extract_section_robust(
            text,
            section_name='Item 1: Business',
            start_patterns=[
                r'ITEM\s+1[\.\:\s]+BUSINESS',
                r'Item\s+1[\.\:\s]+Business',
                r'ITEM\s+1\s*\n+BUSINESS',
            ],
            end_patterns=[
                r'ITEM\s+1A[\.\:\s]+RISK',
                r'Item\s+1A[\.\:\s]+Risk',
            ]
        )
        if item1:
            sections['item_1_business'] = item1
        
        # Item 1A: Risk Factors
        item1a = self.extract_section_robust(
            text,
            section_name='Item 1A: Risk Factors',
            start_patterns=[
                r'ITEM\s+1A[\.\:\s]+RISK\s+FACTORS',
                r'Item\s+1A[\.\:\s]+Risk\s+Factors',
            ],
            end_patterns=[
                r'ITEM\s+1B[\.\:\s]+UNRESOLVED',
                r'Item\s+1B[\.\:\s]+Unresolved',
                r'ITEM\s+2[\.\:\s]+PROPERTIES',
                r'Item\s+2[\.\:\s]+Properties',
            ]
        )
        if item1a:
            sections['item_1a_risk_factors'] = item1a
        
        # Item 7: MD&A
        item7 = self.extract_section_robust(
            text,
            section_name='Item 7: MD&A',
            start_patterns=[
                r'ITEM\s+7[\.\:\s]+MANAGEMENT.*?DISCUSSION',
                r'Item\s+7[\.\:\s]+Management.*?Discussion',
                r'ITEM\s+7\s*\n+MANAGEMENT',
            ],
            end_patterns=[
                r'ITEM\s+7A[\.\:\s]+QUANTITATIVE',
                r'Item\s+7A[\.\:\s]+Quantitative',
                r'ITEM\s+8[\.\:\s]+FINANCIAL\s+STATEMENTS',
                r'Item\s+8[\.\:\s]+Financial\s+Statements',
            ]
        )
        if item7:
            sections['item_7_mda'] = item7
        
        return sections
    
    def extract_section_robust(self, text, section_name, start_patterns, end_patterns):
        """ì„¹ì…˜ ì¶”ì¶œ (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)"""
        
        print(f"\n   ğŸ” Extracting {section_name}...")
        
        # ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
        start_pos = None
        matched_pattern = None
        
        for pattern in start_patterns:
            # DOTALL í”Œë˜ê·¸ë¡œ ì—¬ëŸ¬ ì¤„ ë§¤ì¹­
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                start_pos = match.start()
                matched_pattern = pattern
                print(f"      âœ… Found start at {start_pos:,} (pattern: {pattern[:50]}...)")
                break
        
        if not start_pos:
            print(f"      âš ï¸ Start not found")
            return None
        
        # ë ìœ„ì¹˜ ì°¾ê¸°
        end_pos = None
        search_start = start_pos + 1000  # ì‹œì‘ í›„ 1000ì ë’¤ë¶€í„°
        
        for pattern in end_patterns:
            match = re.search(pattern, text[search_start:], re.IGNORECASE | re.DOTALL)
            if match:
                end_pos = search_start + match.start()
                print(f"      âœ… Found end at {end_pos:,}")
                break
        
        if not end_pos:
            # ëì„ ëª» ì°¾ìœ¼ë©´ 150KB ë˜ëŠ” í…ìŠ¤íŠ¸ ë
            end_pos = min(start_pos + 150000, len(text))
            print(f"      âš ï¸ End not found, using {end_pos:,}")
        
        # ì¶”ì¶œ
        section_text = text[start_pos:end_pos]
        
        # í†µê³„
        char_count = len(section_text)
        word_count = len(section_text.split())
        line_count = len(section_text.split('\n'))
        
        # í˜ì´ì§€ ì¶”ì • (1 page â‰ˆ 3000 chars)
        page_estimate = char_count / 3000
        
        print(f"      ğŸ“Š Extracted:")
        print(f"         Characters: {char_count:,}")
        print(f"         Words: {word_count:,}")
        print(f"         Lines: {line_count:,}")
        print(f"         Pages (est): {page_estimate:.1f}")
        
        return {
            'text': section_text,
            'char_count': char_count,
            'word_count': word_count,
            'line_count': line_count,
            'page_estimate': page_estimate,
            'start_position': start_pos,
            'end_position': end_pos,
        }
    
    def save_parsed_10k(self, ticker, metadata, parsed_data):
        """íŒŒì‹± ê²°ê³¼ ì €ì¥"""
        
        output_file = f'data/parsed_10k_{ticker}.json'
        
        result = {
            'ticker': ticker,
            'collected_at': datetime.now().isoformat(),
            'filing_info': metadata,
            'parsed': parsed_data,
        }
        
        # ì„¹ì…˜ í…ìŠ¤íŠ¸ëŠ” ë³„ë„ íŒŒì¼ë¡œ (ë„ˆë¬´ í¼)
        sections = parsed_data.get('sections', {})
        
        for section_key, section_data in sections.items():
            if section_data and 'text' in section_data:
                # í…ìŠ¤íŠ¸ëŠ” ë³„ë„ íŒŒì¼
                text_file = f'data/section_{ticker}_{section_key}.txt'
                with open(text_file, 'w', encoding='utf-8') as f:
                    f.write(section_data['text'])
                
                # JSONì—ëŠ” íŒŒì¼ ê²½ë¡œë§Œ
                section_data['text_file'] = text_file
                del section_data['text']
        
        # JSON ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Saved to {output_file}")
        
        return output_file


# ì‹¤í–‰
if __name__ == "__main__":
    print("="*80)
    print("ğŸš€ iXBRL 10-K ì™„ì „ íŒŒì„œ")
    print("="*80)
    print()
    print("ğŸ’¡ ëª©í‘œ: SECì˜ ìµœì‹  iXBRL í˜•ì‹ì„ ì™„ì „íˆ íŒŒì‹±")
    print("ğŸ’¡ ì°¨ë³„í™”: ì•„ë¬´ë„ ì‹œë„í•˜ì§€ ì•Šì€ ì™„ì „í•œ 10-K ë°ì´í„°í™”!")
    print("="*80)
    print()
    
    parser = iXBRLParser()
    
    # AAPL í…ŒìŠ¤íŠ¸
    ticker = "AAPL"
    
    print(f"ğŸ“Š Parsing {ticker} 10-K (Latest)...")
    print("="*80)
    
    # 1. ìµœì‹  10-K ë©”íƒ€ë°ì´í„°
    metadata = parser.get_latest_10k(ticker)
    
    if not metadata:
        print("âŒ Failed to get 10-K metadata")
        exit(1)
    
    # 2. HTML ë‹¤ìš´ë¡œë“œ
    html = parser.download_10k_html(metadata['document_url'])
    
    # 3. íŒŒì‹±
    parsed = parser.parse_ixbrl_10k(html)
    
    # 4. ì €ì¥
    output_file = parser.save_parsed_10k(ticker, metadata, parsed)
    
    # 5. ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print(f"ğŸ‰ {ticker} 10-K íŒŒì‹± ì™„ë£Œ!")
    print(f"{'='*80}")
    
    print(f"\nğŸ“Š íŒŒì‹± ê²°ê³¼:")
    print(f"   Filing Date: {metadata['filing_date']}")
    print(f"   Total Text: {parsed['text_length']:,} characters")
    print(f"   Total Lines: {parsed['line_count']:,}")
    
    sections = parsed.get('sections', {})
    print(f"\nğŸ“„ ì¶”ì¶œëœ ì„¹ì…˜: {len(sections)}ê°œ")
    
    for section_name, section_data in sections.items():
        if section_data:
            print(f"\n   {section_name}:")
            print(f"      Words: {section_data['word_count']:,}")
            print(f"      Pages: ~{section_data['page_estimate']:.1f}")
    
    print(f"\n{'='*80}")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"{'='*80}")

