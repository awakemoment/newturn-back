"""
ì™„ì „í•œ 10-K ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘ ë° íŒŒì‹±

ëª©í‘œ: 
- ì „ì²´ 10-K HTML ë‹¤ìš´ë¡œë“œ (100-300 í˜ì´ì§€)
- Itemë³„ ì„¹ì…˜ ì™„ì „ ì¶”ì¶œ
- í…Œì´ë¸” ë°ì´í„° íŒŒì‹±
- êµ¬ì¡°í™”ëœ JSON ì €ì¥

ì´ê²ƒì´ ë‰´í„´ì˜ í•µì‹¬ ì°¨ë³„í™”!
"""
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import time
from urllib.parse import urljoin


class Full10KCollector:
    """ì™„ì „í•œ 10-K ìˆ˜ì§‘ê¸°"""
    
    BASE_URL = "https://www.sec.gov"
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Newturn Investment Platform contact@newturn.ai',
            'Accept-Encoding': 'gzip, deflate',
            'Host': 'www.sec.gov'
        }
    
    def get_cik(self, ticker):
        """í‹°ì»¤ â†’ CIK"""
        url = "https://www.sec.gov/files/company_tickers.json"
        response = requests.get(url, headers=self.headers)
        time.sleep(0.1)
        
        data = response.json()
        for key, company in data.items():
            if company['ticker'].upper() == ticker.upper():
                return str(company['cik_str']).zfill(10)
        return None
    
    def get_latest_10k_filing(self, ticker):
        """ìµœì‹  10-K Filing ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        cik = self.get_cik(ticker)
        if not cik:
            return None
        
        print(f"âœ… {ticker} CIK: {cik}")
        
        # Filing ê²€ìƒ‰
        # 2023ë…„ ì´ì „ 10-K (ì¼ë°˜ HTML í˜•ì‹)
        url = f"{self.BASE_URL}/cgi-bin/browse-edgar"
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': '10-K',
            'dateb': '20231231',  # 2023ë…„ ì´ì „ë§Œ (ì¼ë°˜ HTML)
            'owner': 'exclude',
            'count': '1',
        }
        
        response = requests.get(url, params=params, headers=self.headers)
        time.sleep(0.1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Filing date
        table = soup.find('table', class_='tableFile2')
        if table:
            rows = table.find_all('tr')[1:]  # Skip header
            if rows:
                cells = rows[0].find_all('td')
                filing_date = cells[3].text.strip()
                print(f"âœ… Filing Date: {filing_date}")
        
        # Documents í˜ì´ì§€
        doc_button = soup.find('a', {'id': 'documentsbutton'})
        if not doc_button:
            return None
        
        doc_url = self.BASE_URL + doc_button['href']
        print(f"âœ… Documents: {doc_url}")
        
        return {
            'cik': cik,
            'filing_date': filing_date,
            'documents_url': doc_url
        }
    
    def find_10k_htm_file(self, documents_url):
        """10-K ë©”ì¸ HTML íŒŒì¼ ì°¾ê¸°"""
        response = requests.get(documents_url, headers=self.headers)
        time.sleep(0.1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        table = soup.find('table', class_='tableFile')
        
        if not table:
            print("âŒ No documents table found")
            return None
        
        # ìš°ì„ ìˆœìœ„:
        # 1. ì¼ë°˜ HTML (*.htm, not ix?)
        # 2. ê°€ì¥ í° íŒŒì¼
        
        candidates = []
        rows = table.find_all('tr')[1:]  # Skip header
        
        for row in rows:
            cells = row.find_all('td')
            if len(cells) >= 4:
                doc_type = cells[3].text.strip()
                description = cells[1].text.strip()
                
                if doc_type == '10-K' or '10-K' in description:
                    link = cells[2].find('a')
                    if link:
                        href = link.get('href', '')
                        size_text = cells[4].text.strip() if len(cells) > 4 else '0'
                        
                        # íŒŒì¼ í¬ê¸° íŒŒì‹±
                        size = 0
                        if 'KB' in size_text:
                            size = float(size_text.replace('KB', '').strip())
                        elif 'MB' in size_text:
                            size = float(size_text.replace('MB', '').strip()) * 1024
                        
                        candidates.append({
                            'href': href,
                            'description': description,
                            'size': size,
                            'is_ixbrl': 'ix?' in href
                        })
        
        if not candidates:
            print("âŒ No 10-K file found")
            return None
        
        # ì¼ë°˜ HTML ìš°ì„ , ê·¸ ë‹¤ìŒ í¬ê¸° í° ê²ƒ
        non_ixbrl = [c for c in candidates if not c['is_ixbrl']]
        
        if non_ixbrl:
            # ê°€ì¥ í° ì¼ë°˜ HTML
            best = max(non_ixbrl, key=lambda x: x['size'])
        else:
            # iXBRLì´ë¼ë„ ê°€ì¥ í° ê²ƒ
            best = max(candidates, key=lambda x: x['size'])
        
        full_url = self.BASE_URL + best['href']
        print(f"âœ… 10-K File: {full_url}")
        print(f"   Size: {best['size']:.1f} KB")
        print(f"   iXBRL: {best['is_ixbrl']}")
        
        return full_url
    
    def download_full_10k(self, url):
        """ì „ì²´ 10-K HTML ë‹¤ìš´ë¡œë“œ"""
        print(f"\nğŸ“¥ Downloading full 10-K...")
        
        response = requests.get(url, headers=self.headers)
        time.sleep(0.1)
        
        html = response.text
        
        print(f"âœ… Downloaded: {len(html):,} characters ({len(html)/1024:.1f} KB)")
        
        return html
    
    def parse_full_10k(self, html):
        """ì „ì²´ 10-K íŒŒì‹±"""
        
        print(f"\nğŸ” Parsing 10-K structure...")
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        full_text = soup.get_text(separator='\n', strip=False)
        
        # ë¶ˆí•„ìš”í•œ ì—°ì† ê³µë°± ì œê±°
        full_text = re.sub(r'\n\s*\n\s*\n+', '\n\n', full_text)
        
        print(f"âœ… Full text: {len(full_text):,} characters")
        
        # Part I
        part1 = self.extract_part_i(full_text)
        
        return {
            'full_text_length': len(full_text),
            'part_i': part1,
        }
    
    def extract_part_i(self, text):
        """Part I ì¶”ì¶œ"""
        
        print(f"\nğŸ“„ Extracting Part I...")
        
        sections = {}
        
        # Item 1: Business
        item1 = self.extract_section(
            text,
            start_patterns=[
                r'ITEM\s+1[\.\:]?\s+BUSINESS',
                r'Item\s+1[\.\:]?\s+Business',
            ],
            end_patterns=[
                r'ITEM\s+1A[\.\:]?\s+RISK\s+FACTORS',
                r'Item\s+1A[\.\:]?\s+Risk\s+Factors',
            ],
            section_name='Item 1: Business'
        )
        if item1:
            sections['item_1_business'] = item1
        
        # Item 1A: Risk Factors
        item1a = self.extract_section(
            text,
            start_patterns=[
                r'ITEM\s+1A[\.\:]?\s+RISK\s+FACTORS',
                r'Item\s+1A[\.\:]?\s+Risk\s+Factors',
            ],
            end_patterns=[
                r'ITEM\s+1B[\.\:]?\s+UNRESOLVED\s+STAFF\s+COMMENTS',
                r'Item\s+1B[\.\:]?\s+Unresolved\s+Staff\s+Comments',
                r'ITEM\s+2[\.\:]?\s+PROPERTIES',
                r'Item\s+2[\.\:]?\s+Properties',
            ],
            section_name='Item 1A: Risk Factors'
        )
        if item1a:
            sections['item_1a_risk_factors'] = item1a
        
        # Item 7: MD&A
        item7 = self.extract_section(
            text,
            start_patterns=[
                r'ITEM\s+7[\.\:]?\s+MANAGEMENT.*?DISCUSSION\s+AND\s+ANALYSIS',
                r'Item\s+7[\.\:]?\s+Management.*?Discussion\s+and\s+Analysis',
            ],
            end_patterns=[
                r'ITEM\s+7A[\.\:]?\s+QUANTITATIVE\s+AND\s+QUALITATIVE',
                r'Item\s+7A[\.\:]?\s+Quantitative\s+and\s+Qualitative',
                r'ITEM\s+8[\.\:]?\s+FINANCIAL\s+STATEMENTS',
                r'Item\s+8[\.\:]?\s+Financial\s+Statements',
            ],
            section_name='Item 7: MD&A'
        )
        if item7:
            sections['item_7_mda'] = item7
        
        return sections
    
    def extract_section(self, text, start_patterns, end_patterns, section_name):
        """ì„¹ì…˜ ì¶”ì¶œ (ì‹œì‘/ë íŒ¨í„´ ë§¤ì¹­)"""
        
        # ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
        start_pos = None
        for pattern in start_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                start_pos = match.start()
                print(f"   âœ… Found {section_name} at position {start_pos:,}")
                break
        
        if not start_pos:
            print(f"   âš ï¸ {section_name} not found")
            return None
        
        # ë ìœ„ì¹˜ ì°¾ê¸°
        end_pos = None
        search_start = start_pos + 500  # ì‹œì‘ í›„ 500ì ë’¤ë¶€í„° ê²€ìƒ‰
        
        for pattern in end_patterns:
            match = re.search(pattern, text[search_start:], re.IGNORECASE | re.DOTALL)
            if match:
                end_pos = search_start + match.start()
                print(f"   âœ… End at position {end_pos:,}")
                break
        
        if not end_pos:
            # ëì„ ëª» ì°¾ìœ¼ë©´ 100KBë§Œ
            end_pos = min(start_pos + 100000, len(text))
            print(f"   âš ï¸ End not found, using {end_pos:,}")
        
        section_text = text[start_pos:end_pos]
        
        # í†µê³„
        char_count = len(section_text)
        word_count = len(section_text.split())
        line_count = len(section_text.split('\n'))
        
        print(f"   ğŸ“Š Extracted: {char_count:,} chars, {word_count:,} words, {line_count:,} lines")
        
        return {
            'text': section_text,
            'char_count': char_count,
            'word_count': word_count,
            'line_count': line_count,
        }
    
    def analyze_section_content(self, section_text):
        """ì„¹ì…˜ ë‚´ìš© ë¶„ì„ (í‚¤ì›Œë“œ, íŒ¨í„´ ì¶”ì¶œ)"""
        
        # ì´ê±´ Claude(ë‚˜)ê°€ ì§ì ‘ ë¶„ì„!
        # - ì œí’ˆëª… ì¶”ì¶œ
        # - ìˆ«ì ë°ì´í„° ì¶”ì¶œ
        # - ê²½ìŸì‚¬ ì–¸ê¸‰
        # - ë¦¬ìŠ¤í¬ ì¹´í…Œê³ ë¦¬
        # ë“±
        
        pass
    
    def save_full_10k(self, ticker, data):
        """ì „ì²´ 10-K ì €ì¥"""
        
        output_file = f'data/full_10k_{ticker}.json'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Saved to {output_file}")
        
        # í†µê³„ ì¶œë ¥
        print(f"\n{'='*70}")
        print(f"ğŸ“Š 10-K Statistics for {ticker}")
        print(f"{'='*70}")
        
        part1 = data.get('parsed', {}).get('part_i', {})
        
        for section_key, section_name in [
            ('item_1_business', 'Item 1: Business'),
            ('item_1a_risk_factors', 'Item 1A: Risk Factors'),
            ('item_7_mda', 'Item 7: MD&A'),
        ]:
            section = part1.get(section_key)
            if section:
                print(f"\n{section_name}:")
                print(f"  Characters: {section['char_count']:,}")
                print(f"  Words: {section['word_count']:,}")
                print(f"  Lines: {section['line_count']:,}")
        
        print(f"{'='*70}")


# ì‹¤í–‰
if __name__ == "__main__":
    print("="*70)
    print("ğŸš€ ì™„ì „í•œ 10-K ì‚¬ì—…ë³´ê³ ì„œ ìˆ˜ì§‘")
    print("="*70)
    print()
    print("ğŸ’¡ ëª©í‘œ: ì „ì²´ 10-Kë¥¼ ì™„ì „íˆ íŒŒì‹±í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜")
    print("ğŸ’¡ ì°¨ë³„í™”: ë‹¤ë¥¸ ì„œë¹„ìŠ¤ëŠ” ì•ˆ í•˜ëŠ” ì™„ì „í•œ ë°ì´í„°í™”!")
    print()
    
    collector = Full10KCollector()
    
    # AAPL í…ŒìŠ¤íŠ¸
    ticker = "AAPL"
    
    print(f"ğŸ“Š Collecting {ticker} 10-K...")
    print("="*70)
    
    # 1. Filing ì •ë³´
    filing_info = collector.get_latest_10k_filing(ticker)
    
    if not filing_info:
        print("âŒ Failed to get filing info")
        exit(1)
    
    # 2. HTML íŒŒì¼ ì°¾ê¸°
    htm_url = collector.find_10k_htm_file(filing_info['documents_url'])
    
    if not htm_url:
        print("âŒ Failed to find 10-K HTML file")
        exit(1)
    
    # 3. ë‹¤ìš´ë¡œë“œ
    html = collector.download_full_10k(htm_url)
    
    # 4. íŒŒì‹±
    parsed = collector.parse_full_10k(html)
    
    # 5. ì €ì¥
    result = {
        'ticker': ticker,
        'collected_at': datetime.now().isoformat(),
        'filing_info': filing_info,
        'source_url': htm_url,
        'parsed': parsed,
    }
    
    collector.save_full_10k(ticker, result)
    
    print(f"\nğŸ‰ {ticker} 10-K ìˆ˜ì§‘ ì™„ë£Œ!")

