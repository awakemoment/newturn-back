"""
SEC 10-K ìˆ˜ì§‘ ë° íŒŒì‹± í…ŒìŠ¤íŠ¸

ëª©í‘œ: AAPLì˜ ìµœì‹  10-Kë¥¼ ê°€ì ¸ì™€ì„œ ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ
"""
import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
import time


class SECFilingCollector:
    """SEC EDGARì—ì„œ 10-K ìˆ˜ì§‘"""
    
    BASE_URL = "https://www.sec.gov"
    
    def __init__(self):
        # SECëŠ” User-Agent í•„ìˆ˜!
        self.headers = {
            'User-Agent': 'Newturn AI Investment newturn@example.com',
            'Accept-Encoding': 'gzip, deflate',
            'Host': 'www.sec.gov'
        }
    
    def get_cik(self, ticker):
        """í‹°ì»¤ â†’ CIK ë³€í™˜"""
        # SECì˜ company_tickers.json ì‚¬ìš©
        url = "https://www.sec.gov/files/company_tickers.json"
        
        response = requests.get(url, headers=self.headers)
        time.sleep(0.1)  # Rate limit ì¤€ìˆ˜
        
        data = response.json()
        
        for key, company in data.items():
            if company['ticker'].upper() == ticker.upper():
                cik = str(company['cik_str']).zfill(10)  # 10ìë¦¬ë¡œ
                return cik
        
        return None
    
    def get_latest_10k_url(self, ticker):
        """ìµœì‹  10-K ë¬¸ì„œ URL ì°¾ê¸°"""
        cik = self.get_cik(ticker)
        
        if not cik:
            print(f"âŒ CIK not found for {ticker}")
            return None
        
        print(f"âœ… {ticker} CIK: {cik}")
        
        # Filing ê²€ìƒ‰
        url = f"{self.BASE_URL}/cgi-bin/browse-edgar"
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': '10-K',
            'dateb': '',
            'owner': 'exclude',
            'count': '1',
            'search_text': ''
        }
        
        response = requests.get(url, params=params, headers=self.headers)
        time.sleep(0.1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Documents ë²„íŠ¼ ì°¾ê¸°
        doc_button = soup.find('a', {'id': 'documentsbutton'})
        
        if not doc_button:
            print("âŒ No 10-K found")
            return None
        
        doc_url = self.BASE_URL + doc_button['href']
        print(f"âœ… 10-K Documents page: {doc_url}")
        
        # Documents í˜ì´ì§€ì—ì„œ ì‹¤ì œ HTML íŒŒì¼ ì°¾ê¸°
        response = requests.get(doc_url, headers=self.headers)
        time.sleep(0.1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Tableì—ì„œ 10-K HTML ì°¾ê¸°
        table = soup.find('table', {'class': 'tableFile'})
        
        if table:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 4:
                    doc_type = cells[3].get_text(strip=True)
                    description = cells[1].get_text(strip=True) if len(cells) > 1 else ''
                    
                    # ì¼ë°˜ HTML ë¬¸ì„œ ì°¾ê¸° (iXBRL ì œì™¸)
                    if doc_type == '10-K' and 'htm' in description.lower() and 'ix?' not in str(cells[2]):
                        link = cells[2].find('a')
                        if link and 'ix?' not in link.get('href', ''):
                            filing_url = self.BASE_URL + link['href']
                            print(f"âœ… 10-K HTML: {filing_url}")
                            return filing_url
            
            # ì¼ë°˜ HTML ëª» ì°¾ìœ¼ë©´ ì²« ë²ˆì§¸ 10-K
            print("âš ï¸ No plain HTML found, trying first 10-K...")
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 4:
                    doc_type = cells[3].get_text(strip=True)
                    if doc_type == '10-K':
                        link = cells[2].find('a')
                        if link:
                            filing_url = self.BASE_URL + link['href']
                            print(f"âœ… 10-K (any format): {filing_url}")
                            return filing_url
        
        return None
    
    def get_latest_10k_txt(self, ticker):
        """ìµœì‹  10-K í…ìŠ¤íŠ¸ íŒŒì¼ URL ì°¾ê¸°"""
        cik = self.get_cik(ticker)
        
        if not cik:
            return None
        
        # Filing ê²€ìƒ‰
        url = f"{self.BASE_URL}/cgi-bin/browse-edgar"
        params = {
            'action': 'getcompany',
            'CIK': cik,
            'type': '10-K',
            'dateb': '',
            'owner': 'exclude',
            'count': '1',
        }
        
        response = requests.get(url, params=params, headers=self.headers)
        time.sleep(0.1)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Documents ë²„íŠ¼ ì°¾ê¸°
        doc_button = soup.find('a', {'id': 'documentsbutton'})
        
        if not doc_button:
            return None
        
        # Documents í˜ì´ì§€ URLì—ì„œ accession number ì¶”ì¶œ
        doc_path = doc_button['href']
        # /Archives/edgar/data/320193/000032019325000079/0000320193-25-000079-index.htm
        
        # accession number ì¶”ì¶œ (0000320193-25-000079)
        match = re.search(r'/(\d+-\d+-\d+)-index\.htm', doc_path)
        if match:
            accession = match.group(1)
            # í…ìŠ¤íŠ¸ íŒŒì¼ URL ìƒì„±
            txt_url = f"{self.BASE_URL}/cgi-bin/viewer?action=view&cik={cik}&accession_number={accession}&xbrl_type=v"
            print(f"âœ… 10-K TXT: {txt_url}")
            return txt_url
        
        return None
    
    def download_10k(self, ticker):
        """10-K í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ"""
        # ë¨¼ì € í…ìŠ¤íŠ¸ ë²„ì „ ì‹œë„
        url = self.get_latest_10k_txt(ticker)
        
        if url:
            print(f"\nğŸ“¥ Downloading 10-K (TXT)...")
            response = requests.get(url, headers=self.headers)
            time.sleep(0.1)
            
            print(f"âœ… Downloaded {len(response.text):,} characters")
            return response.text
        
        # ì•ˆ ë˜ë©´ HTML ë²„ì „
        url = self.get_latest_10k_url(ticker)
        
        if not url:
            return None
        
        print(f"\nğŸ“¥ Downloading 10-K (HTML)...")
        response = requests.get(url, headers=self.headers)
        time.sleep(0.1)
        
        print(f"âœ… Downloaded {len(response.text):,} characters")
        
        return response.text
    
    def extract_text_clean(self, html):
        """HTML â†’ ê¹¨ë—í•œ í…ìŠ¤íŠ¸"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Script, style íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'meta', 'link']):
            tag.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text(separator=' ', strip=True)
        
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        return text
    
    def find_section(self, text, item_name, next_item_name=None):
        """íŠ¹ì • Item ì„¹ì…˜ ì°¾ê¸°"""
        
        # Item 1, Item 1A ê°™ì€ íŒ¨í„´ë“¤
        patterns = [
            f"Item {item_name}[.:]",
            f"ITEM {item_name}[.:]",
            f"Item {item_name} ",
            f"ITEM {item_name} ",
        ]
        
        start_pos = None
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                start_pos = match.start()
                print(f"âœ… Found '{item_name}' at position {start_pos}")
                break
        
        if not start_pos:
            print(f"âš ï¸ '{item_name}' not found")
            return None
        
        # ë ìœ„ì¹˜ ì°¾ê¸°
        if next_item_name:
            next_patterns = [
                f"Item {next_item_name}[.:]",
                f"ITEM {next_item_name}[.:]",
            ]
            
            end_pos = None
            for pattern in next_patterns:
                match = re.search(pattern, text[start_pos+100:], re.IGNORECASE)
                if match:
                    end_pos = start_pos + 100 + match.start()
                    break
            
            if end_pos:
                section = text[start_pos:end_pos]
            else:
                # ë‹¤ìŒ Item ëª» ì°¾ìœ¼ë©´ 15000ìë§Œ
                section = text[start_pos:start_pos+15000]
        else:
            section = text[start_pos:start_pos+15000]
        
        return section.strip()
    
    def extract_key_sections(self, html):
        """ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ"""
        
        text = self.extract_text_clean(html)
        print(f"\nğŸ“„ Total text length: {len(text):,} characters")
        
        sections = {}
        
        # Item 1: Business
        print("\nğŸ” Extracting Item 1: Business...")
        business = self.find_section(text, "1", "1A")
        if business:
            sections['business'] = business[:10000]  # ì²˜ìŒ 10000ì
            print(f"   Length: {len(business):,} chars")
        
        # Item 1A: Risk Factors
        print("\nğŸ” Extracting Item 1A: Risk Factors...")
        risks = self.find_section(text, "1A", "1B")
        if risks:
            sections['risk_factors'] = risks[:15000]
            print(f"   Length: {len(risks):,} chars")
        
        # Item 7: MD&A
        print("\nğŸ” Extracting Item 7: MD&A...")
        mda = self.find_section(text, "7", "7A")
        if mda:
            sections['mda'] = mda[:10000]
            print(f"   Length: {len(mda):,} chars")
        
        return sections


# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ SEC 10-K ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    collector = SECFilingCollector()
    
    # AAPL í…ŒìŠ¤íŠ¸
    ticker = "AAPL"
    print(f"\nğŸ“Š Testing with {ticker}")
    
    # 10-K ë‹¤ìš´ë¡œë“œ
    html = collector.download_10k(ticker)
    
    if html:
        # ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ
        sections = collector.extract_key_sections(html)
        
        # ê²°ê³¼ ì €ì¥
        output_file = f"business/newturn-back/data/sec_{ticker}_10k.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'ticker': ticker,
                'collected_at': datetime.now().isoformat(),
                'sections': sections
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\nâœ… Saved to {output_file}")
        print(f"\nğŸ“‹ Sections extracted:")
        for key, value in sections.items():
            print(f"   - {key}: {len(value):,} chars")
    else:
        print("âŒ Failed to download 10-K")

